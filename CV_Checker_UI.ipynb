{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyriakiGeorgiou/KyriakiGeorgiou/blob/main/CV_Checker_UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck4k3M4Ho_Zl",
        "outputId": "cdc53ce8-ec41-44d5-e92b-c8bb486f7e16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradio==3.36.1\n",
            "  Using cached https://gradio-builds.s3.amazonaws.com/a4559af9e1bc6226fc0130d9ab32cc161e99efdd/gradio-3.36.1-py3-none-any.whl (10.1 MB)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (23.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (3.8.5)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.100.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.3.1)\n",
            "Requirement already satisfied: gradio-client>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.24.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.16.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (3.1.2)\n",
            "Requirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (2.1.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (3.7.1)\n",
            "Requirement already satisfied: mdit-py-plugins<=0.3.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (1.22.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (3.9.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (9.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (1.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.25.1)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (2.14.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (2.27.1)\n",
            "Requirement already satisfied: semantic-version in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (2.10.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (0.23.2)\n",
            "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.36.1) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.36.1) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.36.1) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio==3.36.1) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio==3.36.1) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio==3.36.1) (23.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.7->gradio==3.36.1) (4.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.36.1) (3.12.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.14.0->gradio==3.36.1) (4.65.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.36.1) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio==3.36.1) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.36.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio==3.36.1) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.36.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.36.1) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.36.1) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gradio==3.36.1) (3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.36.1) (8.1.6)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio==3.36.1) (0.14.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio==3.36.1) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio==3.36.1) (0.27.0)\n",
            "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.36.1) (0.17.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.36.1) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.36.1) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.36.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.36.1) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.36.1) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio==3.36.1) (3.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio==3.36.1) (3.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio==3.36.1) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio==3.36.1) (1.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio==3.36.1) (1.16.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx->gradio==3.36.1) (1.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U https://gradio-builds.s3.amazonaws.com/a4559af9e1bc6226fc0130d9ab32cc161e99efdd/gradio-3.36.1-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnLeiQX1qJS7"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers==4.30.2\n",
        "!pip install spacy_transformers==1.2.5\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwPZTGf4TZng",
        "outputId": "007065b4-6adf-4154-930c-f8c6140de05a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: spacy-transformers\n",
            "Version: 1.2.5\n",
            "Summary: spaCy pipelines for pre-trained BERT and other transformers\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, spacy, spacy-alignments, srsly, torch, transformers\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show spacy_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u-y6oVcSk6E",
        "outputId": "91b8ad40-41c2-4ad3-dc90-8bf8e8953499"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: transformers\n",
            "Version: 4.30.2\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm\n",
            "Required-by: spacy-transformers\n"
          ]
        }
      ],
      "source": [
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56oF6wig-xRA",
        "outputId": "d2855254-0017-4701-e8e7-ebde9daf6a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: spacy\n",
            "Version: 3.5.0\n",
            "Summary: Industrial-strength Natural Language Processing (NLP) in Python\n",
            "Home-page: https://spacy.io\n",
            "Author: Explosion\n",
            "Author-email: contact@explosion.ai\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, pathy, preshed, pydantic, requests, setuptools, smart-open, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi\n",
            "Required-by: en-core-web-lg, en-core-web-sm, fastai, PassivePy, spacy-transformers\n"
          ]
        }
      ],
      "source": [
        "!pip show spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CziumY7SrL-R"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r https://raw.githubusercontent.com/mitramir55/PassivePy/main/PassivePyCode/PassivePySrc/requirements_lg.txt\n",
        "!pip install PassivePy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFPeAuKPsA2c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uLeETRvipm76"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "aEVLlEbrppLn"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import spacy_transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3PDcH8VVrUPk"
      },
      "outputs": [],
      "source": [
        "from PassivePySrc import PassivePy\n",
        "import os\n",
        "import fitz\n",
        "import re\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E6-8COQx7Glf",
        "outputId": "a1307c06-e86c-41d0-87fe-495fc888c65c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "D7cAXSTSKg7F"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from gradio.inputs import Textbox, File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHj9V8y_Qypr"
      },
      "outputs": [],
      "source": [
        "!python -m spacy validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYhqp0GLQ6jL",
        "outputId": "f4dfaf0b-65ad-438b-d1f0-32082d5aea67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-08-02 08:01:06.309528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.5.0) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "  Attempting uninstall: en-core-web-lg\n",
            "    Found existing installation: en-core-web-lg 3.4.0\n",
            "    Uninstalling en-core-web-lg-3.4.0:\n",
            "      Successfully uninstalled en-core-web-lg-3.4.0\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rEo8AtaUo7kD",
        "outputId": "41768386-910f-4882-c963-e568e42221af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n",
            "<ipython-input-19-4b0a3689bb74>:396: GradioDeprecationWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
            "  File(label=\"Upload your CV in PDF format\")\n",
            "<ipython-input-19-4b0a3689bb74>:396: GradioDeprecationWarning: `optional` parameter is deprecated, and it has no effect\n",
            "  File(label=\"Upload your CV in PDF format\")\n",
            "<ipython-input-19-4b0a3689bb74>:396: GradioDeprecationWarning: `keep_filename` parameter is deprecated, and it has no effect\n",
            "  File(label=\"Upload your CV in PDF format\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "percentages 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action verb use 4\n",
            "strong verbs 10\n",
            "accomplishments 10\n",
            "repetition 4\n",
            "resume length 5\n",
            "filler words 9\n",
            "total bullet points 24\n",
            "bullet points length 6 3\n",
            "passive voice 8\n",
            "personal pronouns 4\n",
            "buzzwords 8\n",
            "resume sections 8\n",
            "date consistency True\n",
            "percentages 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action verb use 4\n",
            "strong verbs 10\n",
            "accomplishments 10\n",
            "repetition 4\n",
            "resume length 5\n",
            "filler words 9\n",
            "total bullet points 24\n",
            "bullet points length 6 3\n",
            "passive voice 8\n",
            "personal pronouns 4\n",
            "buzzwords 8\n",
            "resume sections 8\n",
            "date consistency True\n",
            "percentages 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5 and may not be 100% compatible with the current version (3.4.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "action verb use 4\n",
            "strong verbs 27\n",
            "accomplishments 9\n",
            "repetition -3\n",
            "resume length 10\n",
            "filler words 9\n",
            "total bullet points 24\n",
            "bullet points length 0 0\n",
            "passive voice 9\n",
            "personal pronouns -3\n",
            "buzzwords 0\n",
            "resume sections 7\n",
            "June 2010 – Present\n",
            "June 2010 – Present\n",
            "June 2010 – Present\n",
            "June 2010 – Present\n",
            "June 2010 – Present\n",
            "April 2010 – June 2010\n",
            "April 2010 – June 2010\n",
            "April 2010 – June 2010\n",
            "April 2010 – June 2010\n",
            "April 2010 – June 2010\n",
            "date consistency True\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def read_resume(resume_file):\n",
        "\n",
        "  doc = fitz.open(resume_file)\n",
        "\n",
        "  text = \" \"\n",
        "  page_count = 0\n",
        "  for page in doc:\n",
        "    text = text + str(page.get_text())\n",
        "    page_count += 1\n",
        "\n",
        "  word_count = len(text.split())\n",
        "\n",
        "  return text,word_count,page_count\n",
        "\n",
        "def clean_CV_simple(resume):\n",
        "\n",
        "    cleaned_text = resume.strip()\n",
        "    cleaned_text = cleaned_text.replace(\"•\", \"\")  # Remove bullet points\n",
        "    cleaned_text = cleaned_text.replace(\"·\",\"\")\n",
        "    cleaned_text = cleaned_text.replace(\"_\", \"\")\n",
        "    cleaned_text = cleaned_text.replace(\"➢\", \"\")\n",
        "    cleaned_text = cleaned_text.replace(\"| \",\"\")\n",
        "    cleaned_text = cleaned_text.replace(\",\",\"\").replace(\"(\",\"\").replace(\".\",\"\").replace(\")\",\"\").replace(\"‘\",\"\").replace(\":\",\"\").replace(\"&\",\"\").replace(\";\",\"\").replace(\"/\",\"\").replace(\"-\",\" \")\n",
        "    cleaned_text = cleaned_text.replace(\"etc.\",\"\")\n",
        "\n",
        "    cleaned_text = re.sub(r'\\band\\b', '', cleaned_text, flags=re.IGNORECASE)# Remove standalone \"and\" while preserving it if it is part of a word\n",
        "\n",
        "    cleaned_text = \" \".join(cleaned_text.split())  # Remove unnecessary spaces\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# 1.Quantifying Impact\n",
        "def get_percentages(cleaned_resume):\n",
        "  percentages = re.findall(r'(\\d+)%', cleaned_resume)\n",
        "  percentages_list = []\n",
        "  for percentage in percentages:\n",
        "      percentages_list.append(percentage)\n",
        "  return percentages_list\n",
        "\n",
        "# Helper function for 2\n",
        "def get_verbs(text):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    doc = nlp(text)\n",
        "    return [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# 2. Action Verb Use - Deduct marks\n",
        "def action_verb_use(resume_verbs):\n",
        "\n",
        "  verbs_to_avoid = ['assisted', 'led', 'oversaw', 'utilized' ,'worked','helped','supported']\n",
        "  lemmatized_verbs_to_avoid = []\n",
        "\n",
        "  for word in verbs_to_avoid:\n",
        "      pos = wordnet.VERB\n",
        "      lemma = lemmatizer.lemmatize(word, pos=pos)\n",
        "      lemmatized_verbs_to_avoid.append(lemma)\n",
        "\n",
        "  resume_verbs_to_avoid = [word for word in resume_verbs if word in lemmatized_verbs_to_avoid]\n",
        "  return resume_verbs_to_avoid\n",
        "\n",
        "\n",
        "strong_verbs = ['Accelerated', 'Achieved', 'Attained' ,'Completed', 'Conceived',\n",
        "                'Convinced', 'Discovered' ,'Doubled','Effected','Eliminated' ,'Expanded',\n",
        "                'Expedited' ,'Founded' ,'Improved','Increased','Initiated','Innovated',\n",
        "                'Introduced','Invented','Launched','Mastered','Overcame','Overhauled','Pioneered'\n",
        "                ,'Reduced','Resolved','Revitalised','Spearheaded','Strengthened','Transformed',\n",
        "                'Upgraded','Tripled','Addressed','Advised','Arranged' ,'Authored','Co-authored'\n",
        "                ,'Co-ordinated','Communicated', 'Corresponded','Counselled','Developed',\n",
        "                'Demonstrated','Directed','Drafted','Enlisted','Facilitated','Formulated'\n",
        "                ,'Guided','Influenced','Interpreted','Interviewed','Instructed','Lectured'\n",
        "                ,'Liased','Mediated','Moderated','Motivated','Negotiated','Persuaded'\n",
        "                ,'Presented','Promoted','Proposed','Publicised','Recommended','Reconciled'\n",
        "                ,'Recruited','Resolved','Taught','Trained','Translated','Arranged','Budgeted',\n",
        "                'Composed','Conceived','Conducted','Controlled','Co-ordinated','Eliminated','Improved'\n",
        "                ,'Investigated','Itemised','Modernised','Operated','Organised','Planned','Prepared'\n",
        "                ,'Processed','Produced','Redesigned','Reduced','Refined','Researched','Resolved','Reviewed'\n",
        "                ,'Revised','Scheduled','Simplified','Solved','Streamlined','Transformed','Examined'\n",
        "                ,'Revamped','Acted','Conceptualised','Created','Customised','Designed','Developed'\n",
        "                ,'Directed','Redesigned','Established','Fashioned','Illustrated','Instituted'\n",
        "                ,'Integrated','Performed','Planned','Proved','Revised','Revitalised','Set up',\n",
        "                'Shaped','Streamlined','Structured','Tabulated','Validated', 'Approved',\n",
        "                'Arranged','Catalogued','Classified','Collected','Compiled','Dispatched','Executed','Filed'\n",
        "                ,'Generated','Implemented','Inspected','Monitored','Operated','Ordered','Organised','Prepared'\n",
        "                ,'Processed','Purchased','Recorded','Retrieved','Screened','Specified','Systematised'\n",
        "                ,'administered', 'allocated', 'analysed', 'appraised', 'audited', 'balanced', 'budgeted',\n",
        "                'calculated', 'computed', 'developed', 'managed', 'planned', 'projected', 'researched',\n",
        "                'restructured', 'modelled']\n",
        "\n",
        "# 2.1 Strong Verb Use\n",
        "def strong_verbs_resume(resume_verbs):\n",
        "  lemmatized_strong_verbs = []\n",
        "\n",
        "  for word in strong_verbs:\n",
        "      pos = wordnet.VERB\n",
        "      lemma = lemmatizer.lemmatize(word.lower(), pos=pos)\n",
        "      lemmatized_strong_verbs.append(lemma)\n",
        "\n",
        "\n",
        "  resume_strong_verbs = [word for word in resume_verbs if word in lemmatized_strong_verbs]\n",
        "  return resume_strong_verbs\n",
        "\n",
        "# 3. Accompishments - Avoid Responsibility oriented langauge - Deduct marks\n",
        "def accomplishments(clean_resume):\n",
        "  responsibility_oriented = ['Demonstrated success in', 'Proven results', 'Successful', 'success',\n",
        "                            'Results-driven', 'results-oriented', 'Excellent communication skills',\n",
        "                            'Responsible for', 'Duties included', 'Seasoned', 'Accomplished', 'Worked with',\n",
        "                            'served as']\n",
        "\n",
        "\n",
        "  resume_responsibility_oriented = [phrase for phrase in responsibility_oriented if phrase.lower() in clean_resume.lower()]\n",
        "  return resume_responsibility_oriented\n",
        "\n",
        "# 4. Repetition\n",
        "def repetition(resume_verbs):\n",
        "\n",
        "  duplicates = [item for item, count in Counter(resume_verbs).items() if count > 1]\n",
        "  return duplicates\n",
        "\n",
        "# 5. Resume Length\n",
        "\n",
        "# def resume_length(resume_file):\n",
        "\n",
        "#     doc = fitz.open(re)\n",
        "#     text = \" \"\n",
        "#     page_count = 0\n",
        "#     for page in doc:\n",
        "#         text = text + str(page.get_text())\n",
        "#         page_count +=1\n",
        "\n",
        "#     word_count = len(text.split())\n",
        "\n",
        "\n",
        "#     return word_count,page_count\n",
        "\n",
        "# 6. Filler Words\n",
        "\n",
        "def filler_words(clean_resume):\n",
        "\n",
        "  filler_words = ['various', 'in order to', 'several', 'as well as', 'so that', 'eventually','resourcefully'\n",
        "                  'carried out', 'multiple','quickly','successfully', 'as needed','efficiently',\n",
        "                  'always', 'frequently', 'actively', 'thoroughly', 'efficiently', 'diligently',\n",
        "                  'proficiently', 'significantly']\n",
        "\n",
        "  filler_words_resume = [phrase for phrase in filler_words if phrase in clean_resume]\n",
        "\n",
        "  return filler_words_resume\n",
        "\n",
        "# 7. Total Bullet Points\n",
        "def total_bullet_points(raw_resume):\n",
        "  bullet_symbols = ['•', '·', '➢']\n",
        "  bullet_points_count = sum(raw_resume.count(symbol) for symbol in bullet_symbols)\n",
        "  return bullet_points_count\n",
        "\n",
        "# Load NER model\n",
        "nlp = spacy.load('/content/drive/MyDrive/xml_model')\n",
        "\n",
        "# 8. Bullet Points Length\n",
        "\n",
        "def count_bullet_word_lengths(text):\n",
        "    bullet_word_counts = []\n",
        "    bullet_symbols = ['•', '-', '*']\n",
        "\n",
        "    for symbol in bullet_symbols:\n",
        "        segments = text.split(symbol)\n",
        "        word_counts = [len(re.findall(r'\\b\\w+\\b', segment)) for segment in segments[1:]]\n",
        "        bullet_word_counts.append(word_counts)\n",
        "\n",
        "    return bullet_word_counts\n",
        "\n",
        "# Load Passive Voice Model\n",
        "passivepy = PassivePy.PassivePyAnalyzer(spacy_model = \"en_core_web_lg\")\n",
        "\n",
        "# 9. Active Voice\n",
        "def detect_passive_voice(clean_resume):\n",
        "  passive_voice = passivepy.match_text(clean_resume, full_passive=True, truncated_passive=True)['passive_count']\n",
        "  return passive_voice.item()\n",
        "\n",
        "# 10. Personal Pronouns\n",
        "def find_personal_pronouns(clean_resume):\n",
        "    personal_pronouns = ['I', 'me', 'you', 'he', 'she', 'it', 'we', 'they', 'him', 'her', 'us', 'them','mine','ours']\n",
        "    resume_personal_pronouns = re.findall(r'\\b(?:%s)\\b' % '|'.join(personal_pronouns), clean_resume, flags=re.IGNORECASE)\n",
        "    return resume_personal_pronouns\n",
        "\n",
        "buzzwords = ['Ambitious','Team spirited','Collaborative','Hard working','Honest','Innovative','Autonomous','Passionate',\n",
        "           'Outgoing','Proactive','Creative','Motivated','Determined','Versatile','Goal Orientated','Results Focused',\n",
        "           'Reliable','Problem Solving','Well Organised','Attention to detail', 'Authentic', 'Best of breed','Client focused',\n",
        "          'Committed', 'Communicative','Core competencies','Customer focused', 'Customer service focused','Dedicated','Dependable',\n",
        "          'Detail orientated','Direct','Driven','Dynamic','Easy-going', 'Educated', 'Energetic','Enthusiastic','Excellent communicator',\n",
        "          'Excellent interpersonal skills','Extensive experience','Expert', 'Familiar with', 'Fast learner', 'Fast-paced environment','Flexible','Full of beans',\n",
        "          'Give it my all', 'Gives 110%', 'Go getter', 'Goal directed','Goal driven', 'Goal setter','Goes above and beyond','Goes the extra mile',\n",
        "          'Go-to person','Hands-on','Highly experienced','Highly focused', 'Highly organized','Innovative approach','Inspiring',\n",
        "          'Leader','Loyal','Motivates others','Oversaw a project','People person','Play to strengths','Practical','Proactive',\n",
        "          'Professional','Professional development','Proven track record','Punctual','Rapport builder','Relationship builder',\n",
        "          'Results directed','Results driven','Results orientated', 'Sales driven','Seasoned','Self motivated',\n",
        "          'Self starter','Sincere','Skilled','Specialize','Straightforward','Strategic thinker','Studious','Synergy',\n",
        "          'Task driven','Task orientated','Team leader','Team player','Teamwork','Think outside the box','Thought leader','Transferable skills',\n",
        "          'Trustworthy','Upbeat','Works well in a team','Works well under pressure']\n",
        "\n",
        "# 11. Buzzwords and Clichés\n",
        "def find_buzzwords(clean_resume):\n",
        "\n",
        "  resume_buzzwords = [phrase for phrase in buzzwords if phrase.lower() in clean_resume.lower()]\n",
        "  return resume_buzzwords\n",
        "\n",
        "# 12. Sections\n",
        "\n",
        "def find_sections(doc):\n",
        "    sections = ['Name','Location','Graduation Year','Job Specific Skills', 'Email Address', 'Companies worked at', 'Designation', 'College Name']\n",
        "    count = 0\n",
        "    checked_sections = set()\n",
        "    for ent in doc.ents:\n",
        "        if ent.label_ in sections and ent.label_ not in checked_sections and ent.text.strip() != '':\n",
        "            checked_sections.add(ent.label_)\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "# 13. Date Consistency\n",
        "\n",
        "def check_date_consistency(dates):\n",
        "    formats = [\n",
        "        r'\\d{4}-\\d{4}',  # e.g., 2016-2017\n",
        "        r'\\w+ \\d{4} - \\w+ \\d{4}',  # e.g., June 2016 - June 2017\n",
        "        r'\\w{3} \\d{4} - \\w{3} \\d{4}',  # e.g., Jun 2016 - Jun 2017\n",
        "        r'\\d{2}/\\d{4} to \\d{2}/\\d{4}',  # e.g., 06/2016 to 06/2017\n",
        "        r'\\w+ \\d{4} - \\w+ \\d{4}',  # e.g., Summer 2016 - Autumn 2017\n",
        "    ]\n",
        "\n",
        "    format_matches = [0] * len(formats)\n",
        "\n",
        "    for date in dates:\n",
        "        for i, pattern in enumerate(formats):\n",
        "            print(date)\n",
        "            if re.fullmatch(pattern, date):\n",
        "                format_matches[i] += 1\n",
        "\n",
        "    return all(count == 0 or count == len(dates) for count in format_matches)\n",
        "\n",
        "\n",
        "def output_score(resume):\n",
        "\n",
        "  # overall CV score out of 100\n",
        "  overall_score = 0\n",
        "\n",
        "  impact_score = 0\n",
        "  # read raw resume text\n",
        "  resume_text = read_resume(resume)[0]\n",
        "\n",
        "  # clean resume\n",
        "  clean_resume = clean_CV_simple(resume_text)\n",
        "\n",
        "  ### Start of IMPACT ### 50 marks total\n",
        "\n",
        "  # 1. get the quantifiable metrics\n",
        "  percentages = get_percentages(clean_resume)\n",
        "\n",
        "  impact_score += min(10,len(percentages))\n",
        "  print(\"percentages\", len(percentages))\n",
        "\n",
        "  # 2. Action verb use\n",
        "  # Helper function to retrieve all the verbs from the resume\n",
        "  resume_verbs = get_verbs(clean_resume)\n",
        "\n",
        "  action_verbs = action_verb_use(resume_verbs)\n",
        "\n",
        "  impact_score += min(len(action_verbs),10)\n",
        "  print(\"action verb use\", len(action_verbs))\n",
        "\n",
        "  # 2.1 Strong verb use\n",
        "  strong_verbs = strong_verbs_resume(resume_verbs)\n",
        "\n",
        "  impact_score += min(10, len(strong_verbs))\n",
        "  print(\"strong verbs\", len(strong_verbs))\n",
        "\n",
        "  # 3. Accomplishments\n",
        "  accomplishments_resume = accomplishments(clean_resume)\n",
        "\n",
        "  impact_score += max(0, 10 - len(accomplishments_resume))\n",
        "  print(\"accomplishments\", 10-len(accomplishments_resume))\n",
        "\n",
        "  # 4. Repetition\n",
        "\n",
        "  repetition_resume = repetition(resume_verbs)\n",
        "\n",
        "  impact_score += max(0, 10 - len(repetition_resume))\n",
        "  print(\"repetition\", 10-len(repetition_resume))\n",
        "\n",
        "  #### End Of IMPACT ###\n",
        "\n",
        "  ### Start of BREVITY ### 40 marks\n",
        "\n",
        "  brevity_score = 0\n",
        "  # 5. Resume Length\n",
        "\n",
        "  resume_pages = read_resume(resume)[2]\n",
        "  resume_word_count = read_resume(resume)[1]\n",
        "\n",
        "  if resume_pages < 3:\n",
        "    brevity_score += 5\n",
        "  if resume_word_count >= 450  and resume_word_count <= 900:\n",
        "    brevity_score += 5\n",
        "\n",
        "\n",
        "  print(\"resume length\", brebity_score)\n",
        "\n",
        "  # 6. Filler Words\n",
        "\n",
        "  filler_words_resume = filler_words(clean_resume)\n",
        "\n",
        "  brevity_score += max(0, 10 - len(filler_words_resume))\n",
        "\n",
        "  print(\"filler words\", 10-len(filler_words_resume))\n",
        "\n",
        "  # 7. Total Bullet Points\n",
        "  bullet_points_resume = total_bullet_points(resume_text)\n",
        "\n",
        "  if bullet_points_resume >=12 and bullet_points_resume <=20:\n",
        "    brevity_score += 10\n",
        "  elif bullet_points_resume < 12:\n",
        "    brevity_score += max(0, 10 - bullet_points_resume)\n",
        "  else:\n",
        "    brevity_score += max(0, abs(20-bullet_points_resume))\n",
        "\n",
        "  print(\"total bullet points\", brevity_score)\n",
        "\n",
        "  # 8. Bullet Point Length\n",
        "\n",
        "  bullet_point_length = count_bullet_word_lengths(resume_text)[0]\n",
        "  correct_length = 0\n",
        "  wrong_length = 0\n",
        "\n",
        "  for length in bullet_point_length:\n",
        "    if length >= 10 and length <=30:\n",
        "      correct_length += 1\n",
        "    else:\n",
        "      wrong_length += 1\n",
        "\n",
        "\n",
        "  brevity_score += min(10, correct_length-wrong_length)\n",
        "\n",
        "  print(\"bullet points length\", correct_length,wrong_length)\n",
        "\n",
        "  ### Start of STYLE ### 51 marks\n",
        "\n",
        "  style_score = 0\n",
        "\n",
        "  # 9. Active voice\n",
        "  passive_voice = detect_passive_voice(clean_resume)\n",
        "\n",
        "  style_score += max(0, 10 - passive_voice)\n",
        "\n",
        "  print(\"passive voice\", 10-passive_voice)\n",
        "\n",
        "  # 10. Personal Pronouns\n",
        "  personal_pronouns = find_personal_pronouns(clean_resume)\n",
        "\n",
        "  style_score += min(0, 5-len(personal_pronouns))\n",
        "  print(\"personal pronouns\", 5-len(personal_pronouns))\n",
        "\n",
        "  # 11. Buzzwords and Clichés\n",
        "  buzzwords = find_buzzwords(clean_resume)\n",
        "\n",
        "  style_score += min(0, 10-len(buzzwords))\n",
        "\n",
        "  print(\"buzzwords\", 10-len(buzzwords))\n",
        "\n",
        "  # 12. Sections\n",
        "\n",
        "  doc = nlp(clean_resume)   # Load NER model\n",
        "\n",
        "  resume_sections = find_sections(doc)\n",
        "\n",
        "  style_score += len(resume_sections) * 2\n",
        "\n",
        "  print(\"resume sections\", len(resume_sections))\n",
        "\n",
        "  # 13. Date Consistency\n",
        "\n",
        "  years_experience = []\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == \"Years of Experience\":\n",
        "      years_experience.append(ent.text)\n",
        "\n",
        "  consistent = check_date_consistency(years_experience)\n",
        "\n",
        "  if consistent:\n",
        "    style_score += 5\n",
        "\n",
        "  print(\"date consistency\", consistent)\n",
        "\n",
        "  overall_score = (impact_score + brevity_score + style_score)*100/138\n",
        "  return impact_score,brevity_score,style_score,overall_score\n",
        "\n",
        "\n",
        "inputs = [\n",
        "    File(label=\"Upload your CV in PDF format\")\n",
        "]\n",
        "\n",
        "outputs = [\n",
        "    gr.Text(label = \"Impact Score\"),\n",
        "    gr.Text(label=\"Brevity Score\"),\n",
        "    gr.Text( label=\"Style Score\"),\n",
        "    gr.Text( label=\"Overall Score\")\n",
        "]\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=output_score,\n",
        "    inputs=inputs,\n",
        "    outputs=outputs\n",
        ")\n",
        "\n",
        "interface.launch(share = True,debug = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQeKGgdyKk6e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1p98POfzOZ5IgFInRthrQCaZG7qHQIcr2",
      "authorship_tag": "ABX9TyNswjbAjqRw59Dsk3w3pY1Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}